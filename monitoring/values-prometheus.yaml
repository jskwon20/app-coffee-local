grafana:
  adminPassword: "admin"
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - grafana.192.168.38.140.nip.io
    annotations:
      nginx.ingress.kubernetes.io/ssl-redirect: "false"

prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}
    serviceMonitorNamespaceSelector: {}

additionalPrometheusRulesMap:
  custom-rules:
    groups:
      - name: kubernetes-pod-alert
        rules:
          - alert: PodRestart
            expr: increase(kube_pod_container_status_restarts_total[1m]) > 0
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: "Pod Restart Detected"
              description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted."

          - alert: PodNotReady
            expr: kube_deployment_status_replicas_unavailable > 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: "Deployment Unavailable"
              description: "Deployment {{ $labels.deployment }} has unavailable replicas (Pod Deleted or Not Ready)."

          # [NEW] HTTP 500 ì—ëŸ¬ ê°ì§€ (1ê±´ì´ë¼ë„ ë°œìƒí•˜ë©´ ì¦‰ì‹œ)
          - alert: HttpHighErrorRate
            expr: increase(http_requests_total{status=~"5.."}[1m]) > 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: "Service High Error Rate"
              description: "Backend API is returning 500 errors."

alertmanager:
  alertmanagerSpec:
    secrets:
      - alertmanager-slack-creds

  config:
    global:
      resolve_timeout: 5m
      slack_api_url_file: "/etc/alertmanager/secrets/alertmanager-slack-creds/url"
    route:
      group_by: ['alertname', 'job']
      group_wait: 30s
      group_interval: 5m
      receiver: 'slack-notifications'
      routes:
        # [NEW] ì‹œë„ëŸ¬ìš´ ê¸°ë³¸ ì•Œë¦¼ì€ 'null' (ì“°ë ˆê¸°í†µ)ìœ¼ë¡œ ë³´ëƒ„
        - match_re:
            alertname: "Watchdog|NodeClockNotSynchronising|TargetDown|etcdInsufficientMembers"
          receiver: 'null'

        - match:
            alertname: PodRestart
          receiver: 'slack-notifications'
    receivers:
    - name: 'null'
    - name: 'slack-notifications'
      slack_configs:
      - channel: '#sre_monitoring'
        send_resolved: true
        title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonAnnotations.summary }}'
        text: >-
          {{ range .Alerts }}
            ğŸ“¢ *{{ .Annotations.summary }}*
            {{ .Annotations.description }}

            ğŸ“‹ *ìƒì„¸ ì •ë³´*
            â€¢ *ì‹¬ê°ë„:* `{{ .Labels.severity }}`
            â€¢ *ë°œìƒ:* `{{ (.StartsAt.Add 32400000000000).Format "15:04:05" }}` (KST)
            {{ if eq .Status "resolved" }}â€¢ *í•´ê²°:* `{{ (.EndsAt.Add 32400000000000).Format "15:04:05" }}` (KST)
            â€¢ *ì†Œìš” ì‹œê°„:* `{{ .EndsAt.Sub .StartsAt }}`{{ end }}
            {{ if .Labels.namespace }}â€¢ *Namespace:* `{{ .Labels.namespace }}`{{ end }}
            {{ if .Labels.deployment }}â€¢ *Deployment:* `{{ .Labels.deployment }}`{{ end }}
            {{ if eq .Labels.alertname "PodRestart" }}â€¢ *Pod:* `{{ .Labels.pod }}`{{ end }}
            {{ if .Labels.alertname }}â€¢ *Rule:* `{{ .Labels.alertname }}`{{ end }}
            
            ----------------------------------------------
          {{ end }}
